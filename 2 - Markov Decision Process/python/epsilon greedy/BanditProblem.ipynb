{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robitussin/CCRNFLRL/blob/main/BanditProblem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXQZ2IVyfV4E"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"\n",
        "Epsilon-greedy action value method for solving the multi-armed bandit problem\n",
        "The multi-armed Bandit Problem is also known as the k-armed bandit problem\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "\n",
        "class BanditProblem(object):\n",
        "    # trueActionValues - means of the normal distributions used to generate random rewards\n",
        "    # the number of arms is equal to the number of entries in the trueActionValues\n",
        "    # epsilon - epsilon probability value for selecting non-greedy actions\n",
        "    # totalSteps - number of tatal steps used to simulate the solution of the mu\n",
        "    \n",
        "   \n",
        "    \n",
        "    def __init__(self,trueActionValues, epsilon, totalSteps):\n",
        "        \n",
        "        \n",
        "        # number of arms\n",
        "        self.armNumber=np.size(trueActionValues)\n",
        "        \n",
        "        # probability of ignoring the greedy selection and selecting \n",
        "        # an arm by random \n",
        "        self.epsilon=epsilon  \n",
        "        \n",
        "        #current step \n",
        "        self.currentStep=0\n",
        "        \n",
        "        #this variable tracks how many times a particular arm is being selected\n",
        "        self.howManyTimesParticularArmIsSelected=np.zeros(self.armNumber)\n",
        "        \n",
        "        #total steps\n",
        "        self.totalSteps=totalSteps\n",
        "        \n",
        "        # true action values that are expectations of rewards for arms\n",
        "        self.trueActionValues=trueActionValues\n",
        "        \n",
        "        \n",
        "        # vector that stores mean rewards of every arm\n",
        "        self.armMeanRewards=np.zeros(self.armNumber)\n",
        "        \n",
        "        # variable that stores the current value of reward\n",
        "        self.currentReward=0;\n",
        "        \n",
        "        # mean reward \n",
        "        self.meanReward=np.zeros(totalSteps+1)\n",
        "        \n",
        "    # select actions according to the epsilon-greedy approach\n",
        "    def selectActions(self):\n",
        "        # draw a real number from the uniform distribution on [0,1]\n",
        "        # this number is our probability of performing greedy actions\n",
        "        # if this probabiligy is larger than epsilon, we perform greedy actions\n",
        "        # otherwise, we randomly select an arm \n",
        "        \n",
        "        probabilityDraw=np.random.rand()\n",
        "         \n",
        "              \n",
        "        # in the initial step, we select a random arm since all the mean rewards are zero\n",
        "        # we also select a random arm if the probability is smaller than epsilon\n",
        "        if (self.currentStep==0) or (probabilityDraw<=self.epsilon):\n",
        "            selectedArmIndex=np.random.choice(self.armNumber)\n",
        "                   \n",
        "        # we select the arm that has the largest past mean reward\n",
        "        if (probabilityDraw>self.epsilon):\n",
        "            selectedArmIndex=np.argmax(self.armMeanRewards)\n",
        "            \n",
        "        # increase the step value\n",
        "        \n",
        "        self.currentStep=self.currentStep+1\n",
        "        \n",
        "        # take a record that the particular arm is selected \n",
        "        \n",
        "        self.howManyTimesParticularArmIsSelected[selectedArmIndex]=self.howManyTimesParticularArmIsSelected[selectedArmIndex]+1\n",
        "        \n",
        "             \n",
        "        # draw from the probability distribution of the selected arm the reward\n",
        "        \n",
        "        self.currentReward=np.random.normal(self.trueActionValues[selectedArmIndex],2)\n",
        "        \n",
        "        # update the estimate of the mean reward\n",
        "        \n",
        "        self.meanReward[self.currentStep]=self.meanReward[self.currentStep-1]+(1/(self.currentStep))*(self.currentReward-self.meanReward[self.currentStep-1])\n",
        "        \n",
        "        # update the estimate of the mean reward for the selected arm \n",
        "        \n",
        "        self.armMeanRewards[selectedArmIndex]=self.armMeanRewards[selectedArmIndex]+(1/(self.howManyTimesParticularArmIsSelected[selectedArmIndex]))*(self.currentReward-self.armMeanRewards[selectedArmIndex])\n",
        "    \n",
        "    # run the simulation\n",
        "    def playGame(self):\n",
        "        for i in range(self.totalSteps):\n",
        "            self.selectActions()\n",
        "        \n",
        "        \n",
        "    # reset all the variables to the original state \n",
        "    def clearAll(self):\n",
        "         #current step \n",
        "        self.currentStep=0\n",
        "         #this variable tracks how many times a particular arm is being selected\n",
        "        self.howManyTimesParticularArmIsSelected=np.zeros(self.armNumber)\n",
        "        \n",
        "         # vector that stores mean rewards of every arm\n",
        "        self.armMeanRewards=np.zeros(self.armNumber)\n",
        "        \n",
        "        # variable that stores the current value of reward\n",
        "        self.currentReward=0;\n",
        "        \n",
        "        # mean reward \n",
        "        self.meanReward=np.zeros(self.totalSteps+1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNq8lXPndIfGCUEIQ5kNnf6",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
